{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globus and Funcx \n",
    "\n",
    "Using Globus and Funcx to automate the process of uploading data to NeSI, running something on NeSI and then copying results back.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "* Globus account\n",
    "* Globus personal endpoint running on the machine you are executing this notebook\n",
    "* Globus shared collection created on NeSI on the nobackup filesystem\n",
    "* NeSI account, username and 2nd factor, for authenticating with the NeSI Globus endpoint and running a Funcx endpoint\n",
    "\n",
    "Authentication/setup Steps:\n",
    "\n",
    "1. Connect to NeSI cluster and start a funcx endpoint there\n",
    "2. Globus authentication (including FuncX) on local machine\n",
    "3. Start funcX client locally\n",
    "4. Create Globus transfer client\n",
    "5. Connect to source Globus endpoint\n",
    "6. Connect to NeSI Globus endpoint\n",
    "\n",
    "Processing Steps:\n",
    "\n",
    "7. Transfer input data to NeSI using Globus\n",
    "8. Run the workflow using funcX\n",
    "9. Copy result back using Globus\n",
    "\n",
    "Steps 1,2 and 6 above require authentication. \n",
    "\n",
    "The tokens generated in step 2 on the local machine are stored in a file and reused, so you should only need to authenticate the first time.\n",
    "\n",
    "Connecting to NeSI Globus endpoint in step 6 requires NeSI 2 factor authentication and you only remain authenticated for ~24 hours.\n",
    "\n",
    "References:\n",
    "\n",
    "* [Globus tutorial](https://globus-sdk-python.readthedocs.io/en/stable/tutorial.html)\n",
    "* [funcX endpoint documentation](https://funcx.readthedocs.io/en/latest/endpoints.html)\n",
    "* [fair-research-login](https://github.com/fair-research/native-login)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start funcx endpoint on NeSI\n",
    "\n",
    "### Install and configure funcx endpoint if you have not done it before\n",
    "\n",
    "Connect to a Mahuika login node by SSH and run the following commands to install funcx:\n",
    "\n",
    "```sh\n",
    "ssh mahuika\n",
    "module load Python\n",
    "pip install --user funcx funcx_endpoint\n",
    "funcx-endpoint configure\n",
    "```\n",
    "\n",
    "During the final command you will be asked to authenticate with Globus Auth so that your endpoint can be made available to funcx running outside of NeSI.\n",
    "\n",
    "For more details see: https://funcx.readthedocs.io/en/latest/endpoints.html.\n",
    "\n",
    "### Start the funcx endpoint on NeSI\n",
    "\n",
    "A default endpoint profile is created during the configure step above, which will suffice for us. We will be using funcx to submit jobs to Slurm or check the status of submitted jobs; no computationally expensive tasks should run directly on the endpoint itself.\n",
    "\n",
    "```sh\n",
    "# we are still on the Mahuika login node here...\n",
    "funcx-endpoint start\n",
    "```\n",
    "\n",
    "Now list your endpoints, confirm that the *default* endpoint is \"Active\" and make a note of your endpoint ID:\n",
    "\n",
    "```sh\n",
    "funcx-endpoint list\n",
    "+---------------+-------------+--------------------------------------+\n",
    "| Endpoint Name |   Status    |             Endpoint ID              |\n",
    "+===============+=============+======================================+\n",
    "| default       | Active      | 3abf6696-8ba4-4ac8-be69-c6c24031373d |\n",
    "+---------------+-------------+--------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store your funcx endpoint id here\n",
    "funcx_endpoint = \"3abf6696-8ba4-4ac8-be69-c6c24031373d\"  # my default endpoint on NeSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Globus authentication (including FuncX) on local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register an app with Globus, if you haven't done it already\n",
    "\n",
    "Note: I think this is a one off, you can reuse the same client id.\n",
    "\n",
    "> Navigate to the [Developer Site](https://developers.globus.org/) and select “Register your app with Globus.” You will be prompted to login – do so with the account you wish to use as your app’s administrator..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifier for the app we created on globus website above, can be reused\n",
    "CLIENT_ID = \"6ffc9c02-cf62-4268-a695-d9d100181962\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use fair-research-login to authenticate once with Globus for both FuncX and Globus transfer\n",
    "\n",
    "The first time you have to authenticate, then token is stored in mytokens.json and loaded from there on subsequent calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fair_research_login import NativeClient, JSONTokenStorage\n",
    "\n",
    "cli = NativeClient(\n",
    "    client_id=CLIENT_ID,\n",
    "    token_storage=JSONTokenStorage('mytokens.json'),  # save/load tokens here\n",
    "    app_name=\"FuncX/Globus NeSI Demo\",\n",
    ")\n",
    "\n",
    "# get the requested scopes\n",
    "search_scope = \"urn:globus:auth:scope:search.api.globus.org:all\"  # for FuncX\n",
    "funcx_scope = \"https://auth.globus.org/scopes/facd7ccc-c5f4-42aa-916b-a0e270e2c2a9/all\"  # for FuncX\n",
    "openid_scope = \"openid\"  # for FuncX\n",
    "transfer_scope = \"urn:globus:auth:scope:transfer.api.globus.org:all\"  # for Globus transfer\n",
    "tokens = cli.login(\n",
    "    refresh_tokens=True,\n",
    "    requested_scopes=[openid_scope, search_scope, funcx_scope, transfer_scope]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authorisers for requested scopes\n",
    "authorisers = cli.get_authorizers_by_scope(requested_scopes=[openid_scope, funcx_scope, search_scope, transfer_scope])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Start funcX client locally\n",
    "\n",
    "Start the funcX client locally so we can submit jobs to the NeSI funcX endpoint we just created. This will also require authentication with Globus Auth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcx.sdk.client import FuncXClient\n",
    "\n",
    "fxc = FuncXClient(\n",
    "    fx_authorizer=authorisers[funcx_scope],\n",
    "    search_authorizer=authorisers[search_scope],\n",
    "    openid_authorizer=authorisers[openid_scope],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcx.sdk.executor import FuncXExecutor\n",
    "\n",
    "# create a funcX executor\n",
    "funcx_executor = FuncXExecutor(fxc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Globus transfer client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import globus_sdk\n",
    "\n",
    "tc = globus_sdk.TransferClient(authorizer=authorisers[transfer_scope])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Globus Endpoints:\n",
      "  - [6890f1a4-3f21-11eb-b55a-02d9497ca481] cdjs-desktop\n",
      "  - [d5a64768-cf1c-11eb-bde7-5111456017d9] laptoptestshare\n",
      "  - [d1afe264-ceee-11eb-8172-2bdce096500e] nesitestshare\n",
      "  - [0ad12a38-40d9-11ec-beaf-59ff7db44e9b] temptest\n",
      "  - [106cd58b-e35c-11eb-832a-45cc1b8ccd4a] TestShareOnNeSI01\n",
      "  - [2d63f434-e35c-11eb-832a-45cc1b8ccd4a] TestShareOnNeSI02\n",
      "  - [1fdfb7aa-544e-11eb-87b7-02187389bd35] WorkLaptop\n"
     ]
    }
   ],
   "source": [
    "print(\"My Globus Endpoints:\")\n",
    "for ep in tc.endpoint_search(filter_scope=\"my-endpoints\"):\n",
    "    print(\"  - [{}] {}\".format(ep[\"id\"], ep[\"display_name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connect to source Globus endpoint\n",
    "\n",
    "Connect to the personal endpoint you have on your local machine (e.g. where you are running this notebook).\n",
    "\n",
    "You should not need to do any authentication since Globus will use the token you generated above.\n",
    "\n",
    "We list the files in the input data directory to check our access is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint activation succeeded: AlreadyActivated\n",
      "Source files:\n",
      "  apoa1.namd (file)\n",
      "  apoa1.pdb (file)\n",
      "  apoa1.psf (file)\n",
      "  par_all22_popc.xplor (file)\n",
      "  par_all22_prot_lipid.xplor (file)\n",
      "  run.sl (file)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# put your Globus endpoint id here:\n",
    "#src_endpoint = \"1fdfb7aa-544e-11eb-87b7-02187389bd35\"\n",
    "src_endpoint = \"6890f1a4-3f21-11eb-b55a-02d9497ca481\"\n",
    "\n",
    "# paths to input and output\n",
    "src_base_path = os.getcwd()\n",
    "src_input_path = os.path.join(src_base_path, \"input\")\n",
    "src_output_path = os.path.join(src_base_path, \"output\")\n",
    "\n",
    "# activate the endpoint\n",
    "res_src_ep = tc.endpoint_autoactivate(src_endpoint, if_expires_in=3600)\n",
    "if res_src_ep['code'] == 'AutoActivationFailed':\n",
    "    print(\"Endpoint activation failed!\")\n",
    "    print(res_src_ep)\n",
    "else:\n",
    "    print(f\"Endpoint activation succeeded: {res_src_ep['code']}\")\n",
    "    \n",
    "    # list the source directory\n",
    "    print(\"Source files:\")\n",
    "    for entry in tc.operation_ls(src_endpoint, path=src_input_path):\n",
    "        print(f'  {entry[\"name\"]} ({entry[\"type\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Connect to the NeSI Globus endpoint\n",
    "\n",
    "Create a guest collection on the NeSI endpoint, so that we don't need to do the NeSI two factor authentication repeatedly, we can just use Globus auth.\n",
    "\n",
    "Navigate to a directory under */nesi/nobackup/[project_code]/*, click sharing and add a shared collection. Make a note of the \"Endpoint UUID\". Also store the full path on NeSI to the shared collection you just created (`nesi_path`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nesi_endpoint = \"3064bb28-e940-11e8-8caa-0a1d4c5c824a\"  # NeSI endpoint\n",
    "#nesi_endpoint = \"cc45cfe3-21ae-4e31-bad4-5b3e7d6a2ca1\"  # NeSI v5 endpoint\n",
    "#nesi_path = \"/nesi/nobackup/nesi99999/csco212/cer_instrument_data/funcx/test-workflow\"\n",
    "nesi_endpoint = \"f456a507-3c5b-41b9-9d7f-2315b9fed386\"  # shared collection on NeSI\n",
    "nesi_path = \"/nesi/nobackup/nesi99999/csco212/funcx_demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Endpoint is already activated and does not expire before the requested if_expires_in.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activate the endpoint\n",
    "res_nesi_ep = tc.endpoint_autoactivate(nesi_endpoint, if_expires_in=3600)\n",
    "assert res_nesi_ep['code'] != 'AutoActivationFailed'\n",
    "res_nesi_ep[\"message\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transfer input data to NeSI\n",
    "\n",
    "First we make a directory name that the simulation will be stored under, then copy the data under that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: 20211129T131824\n"
     ]
    }
   ],
   "source": [
    "# make a directory for running under\n",
    "from datetime import datetime\n",
    "\n",
    "# get a unique name for this run\n",
    "workdirbase = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "workdirname = workdirbase\n",
    "got_dirname = False\n",
    "existing_names = [item[\"name\"] for item in tc.operation_ls(nesi_endpoint, path=\"/\")]\n",
    "count = 0\n",
    "while not got_dirname:\n",
    "    # check the directory does not already exist\n",
    "    if workdirname in existing_names:\n",
    "        count += 1\n",
    "        workdirname = f\"{workdirbase}.{count:06d}\"\n",
    "    else:\n",
    "        got_dirname = True\n",
    "print(f\"Directory: {workdirname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory on NeSI will be (relative to collection): /20211129T131824\n",
      "Working directory on NeSI will be (full): /nesi/nobackup/nesi99999/csco212/funcx_demo/20211129T131824\n",
      "task_id = dfd70b1e-50a9-11ec-a9ca-91e0e7641750\n",
      "waiting for transfer to complete...\n",
      "waiting for transfer to complete...\n",
      "waiting for transfer to complete...\n",
      "waiting for transfer to complete...\n",
      "waiting for transfer to complete...\n",
      "waiting for transfer to complete...\n",
      "waiting for transfer to complete...\n",
      "waiting for transfer to complete...\n",
      "transfer to NeSI is complete\n"
     ]
    }
   ],
   "source": [
    "# initiate the data transfer to NeSI\n",
    "tdata = globus_sdk.TransferData(tc, src_endpoint,\n",
    "                                    nesi_endpoint,\n",
    "                                    label=\"Sending input data to NeSI\",\n",
    "                                    sync_level=\"checksum\")\n",
    "\n",
    "# add the input files to the transfer\n",
    "nesi_relative_path = \"/\" + workdirname\n",
    "nesi_full_path = nesi_path + nesi_relative_path\n",
    "print(f\"Working directory on NeSI will be (relative to collection): {nesi_relative_path}\")\n",
    "print(f\"Working directory on NeSI will be (full): {nesi_full_path}\")\n",
    "tdata.add_item(src_input_path, nesi_relative_path,\n",
    "               recursive=True)\n",
    "\n",
    "# actually start the transfer\n",
    "transfer_result = tc.submit_transfer(tdata)\n",
    "task_id = transfer_result[\"task_id\"]\n",
    "print(\"task_id =\", transfer_result[\"task_id\"])\n",
    "\n",
    "# the task id can be used to refer to this transfer\n",
    "# for example, here we wait for the data transfer to complete\n",
    "while not tc.task_wait(task_id, timeout=10, polling_interval=10):\n",
    "    print(\"waiting for transfer to complete...\")\n",
    "print(\"transfer to NeSI is complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also login to Globus web interface and see the status of your transfer there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run the processing using funcX\n",
    "\n",
    "Two functions are called using FuncX:\n",
    "\n",
    "1. Submit job to Slurm\n",
    "2. Check Slurm job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FuncX endpoint id: 3abf6696-8ba4-4ac8-be69-c6c24031373d\n"
     ]
    }
   ],
   "source": [
    "print(f\"FuncX endpoint id: {funcx_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple test function that returns the hostname where the endpoint is running, just as a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing done? False\n",
      "processing done? True\n",
      "mahuika01\n"
     ]
    }
   ],
   "source": [
    "# test function to see if things are working\n",
    "def test_function():\n",
    "    import socket\n",
    "    return socket.gethostname()\n",
    "\n",
    "# With the executor, functions are auto-registered\n",
    "future = funcx_executor.submit(test_function, endpoint_id=funcx_endpoint)\n",
    "\n",
    "# You can check status of your task without blocking\n",
    "print(\"processing done?\", future.done())\n",
    "\n",
    "# Block and wait for the result:\n",
    "result = future.result()\n",
    "\n",
    "print(\"processing done?\", future.done())\n",
    "\n",
    "print(f\"FuncX endpoint is running on: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the 2 Slurm functions for interacting with Slurm (if the Slurm API was available we could use that instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that submits a job to Slurm (assumes submit script and other required inputs were uploaded via Globus)\n",
    "def submit_slurm_job(submit_script, work_dir=None):\n",
    "    \"\"\"Runs the given command in a Slurm job.\"\"\"\n",
    "    # have to load modules within the function\n",
    "    import os\n",
    "    import subprocess\n",
    "    \n",
    "    # change to working directory\n",
    "    if work_dir is not None and os.path.isdir(work_dir):\n",
    "        os.chdir(work_dir)\n",
    "        \n",
    "    print(os.listdir())\n",
    "    \n",
    "    # submit the Slurm job and return the job id\n",
    "    submit_cmd = f'sbatch --priority=9999 {submit_script}'\n",
    "    with open(\"submit_cmd.txt\", \"w\") as fh:\n",
    "        fh.write(submit_cmd + \"\\n\")\n",
    "    output = subprocess.check_output(submit_cmd, shell=True, universal_newlines=True)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that checks Slurm job status\n",
    "def check_slurm_job_status(jobid):\n",
    "    \"\"\"Check Slurm job status.\"\"\"\n",
    "    # have to load modules within the function\n",
    "    import subprocess\n",
    "    \n",
    "    # query the status of the job using sacct\n",
    "    cmd = f'sacct -j {jobid} -X -o State -n'\n",
    "    output = subprocess.check_output(cmd, shell=True, universal_newlines=True)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job to Slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job submitted: 23296012\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# With the executor, functions are auto-registered\n",
    "future = funcx_executor.submit(submit_slurm_job, \"run.sl\", endpoint_id=funcx_endpoint, work_dir=nesi_full_path)\n",
    "\n",
    "# Block and wait for the result:\n",
    "try:\n",
    "    result = future.result()\n",
    "except subprocess.CalledProcessError as exc:\n",
    "    print(\"submitting job failed:\")\n",
    "    print(f\"    return code: {exc.returncode}\")\n",
    "    print(f\"    cmd: {exc.cmd}\")\n",
    "    print(f\"    output: {exc.output}\")\n",
    "\n",
    "# get the Slurm Job ID\n",
    "jobid = result.split()[-1]\n",
    "print(f\"Job submitted: {jobid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking Slurm job status via funcX: RUNNING\n",
      "checking Slurm job status via funcX: RUNNING\n",
      "checking Slurm job status via funcX: RUNNING\n",
      "checking Slurm job status via funcX: RUNNING\n",
      "checking Slurm job status via funcX: RUNNING\n",
      "checking Slurm job status via funcX: RUNNING\n",
      "checking Slurm job status via funcX: COMPLETED\n",
      "Job finished\n"
     ]
    }
   ],
   "source": [
    "job_finished = False\n",
    "while not job_finished:\n",
    "    future = funcx_executor.submit(check_slurm_job_status, jobid, endpoint_id=funcx_endpoint)\n",
    "    print(\"checking Slurm job status via funcX: \", end=\"\")\n",
    "    result = future.result()\n",
    "    job_status = result.strip()\n",
    "    print(job_status)\n",
    "    if job_status not in (\"RUNNING\", \"PENDING\"):  # TODO: check possible statuses\n",
    "        job_finished = True\n",
    "    time.sleep(5)\n",
    "print(\"Job finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Copy result back using Globus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_id = 31d9412a-50aa-11ec-a9ca-91e0e7641750\n",
      "waiting for transfer to complete...\n",
      "waiting for transfer to complete...\n",
      "waiting for transfer to complete...\n",
      "waiting for transfer to complete...\n",
      "transfer from NeSI is complete: /home/cdjs/DocumentsSync/work/projects/funcx_globus_demo/funcx-globus-nesi-demo/output/20211129T131824\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from_path = nesi_relative_path\n",
    "to_path = os.path.join(src_output_path, workdirname)\n",
    "tdata = globus_sdk.TransferData(tc, nesi_endpoint,\n",
    "                                    src_endpoint,\n",
    "                                    label=\"Retrieving results from NeSI\",\n",
    "                                    sync_level=\"checksum\")\n",
    "tdata.add_item(from_path, to_path, recursive=True)\n",
    "transfer_result = tc.submit_transfer(tdata)\n",
    "task_id = transfer_result[\"task_id\"]\n",
    "print(\"task_id =\", transfer_result[\"task_id\"])\n",
    "\n",
    "# wait for the data transfer to complete\n",
    "while not tc.task_wait(task_id, timeout=10, polling_interval=10):\n",
    "    print(\"waiting for transfer to complete...\")\n",
    "print(f\"transfer from NeSI is complete: {to_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes about FuncX so far\n",
    "\n",
    "* above uses FuncX just to submit a Slurm job and then poll for completion (could also be done with Slurm API if that was made available)\n",
    "* researcher needs to manually run a funcx endpoint on NeSI (and keep it running there)\n",
    "  - eventually should be integrated with Globus federated endpoint?\n",
    "  - this runs an endpoint on the login node\n",
    "  - could be a pain if the endpoint is killed for some reason and the user needs to reconnect and start it again\n",
    "* FuncX does know about Slurm too, so you could set FuncX up to directly run your function in a Slurm job without having to submit anything separately, see snippet from an endpoint config.py:\n",
    "  ```sh\n",
    "    from funcx_endpoint.endpoint.utils.config import Config\n",
    "    from parsl.providers import LocalProvider, SlurmProvider\n",
    "\n",
    "    config = Config(\n",
    "        scaling_enabled=True,\n",
    "        provider=SlurmProvider(\n",
    "            \"large\",\n",
    "            min_blocks=1,\n",
    "            max_blocks=1,\n",
    "            nodes_per_block=1,\n",
    "            cores_per_node=2,\n",
    "            mem_per_node=16,\n",
    "            exclusive=False,\n",
    "            cmd_timeout=120,\n",
    "            walltime='2:00:00',\n",
    "        ),\n",
    "        #max_workers_per_node=2,\n",
    "        funcx_service_address='https://api.funcx.org/v1'\n",
    "    )\n",
    "  ```\n",
    "* reasons for not using FuncX SlurmProvider directly currently\n",
    "  - funcx currently has no way to know how much work a function may involve\n",
    "    - could lead to failures due to wall time exceeded, etc.\n",
    "  - not \"elastic\"\n",
    "    - have to start a new endpoint if need more resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
