{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globus and Funcx \n",
    "\n",
    "Using Globus and Funcx to automate the process of uploading data to NeSI, running something on NeSI and then copying results back.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "* Globus account\n",
    "* NeSI account\n",
    "\n",
    "Authentication/setup Steps:\n",
    "\n",
    "1. Start a FuncX endpoint on NeSI\n",
    "2. Create a Globus guest collection on NeSI\n",
    "3. Globus authentication on local machine\n",
    "4. Start funcX client locally\n",
    "5. Connect to our Globus guest connection on NeSI\n",
    "6. Configure HTTPS uploads/downloads for our NeSI guest collection\n",
    "\n",
    "Processing Steps:\n",
    "\n",
    "7. Transfer input data to NeSI using Globus\n",
    "8. Run the workflow using funcX\n",
    "9. Copy results back using Globus\n",
    "\n",
    "The tokens generated during step 3 on the local machine are stored in a file and reused, so you should only need to authenticate the first time you run this notebook.\n",
    "\n",
    "References:\n",
    "\n",
    "* [Globus tutorial](https://globus-sdk-python.readthedocs.io/en/stable/tutorial.html)\n",
    "* [funcX endpoint documentation](https://funcx.readthedocs.io/en/latest/endpoints.html)\n",
    "* [fair-research-login](https://github.com/fair-research/native-login)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start a funcx endpoint on NeSI\n",
    "\n",
    "### Install and configure funcx endpoint if you have not done it before\n",
    "\n",
    "Connect to a Mahuika login node by SSH and run the following commands to install funcx:\n",
    "\n",
    "```sh\n",
    "ssh mahuika\n",
    "module load Python\n",
    "pip install --user funcx funcx_endpoint\n",
    "funcx-endpoint configure\n",
    "```\n",
    "\n",
    "During the final command you will be asked to authenticate with Globus Auth so that your endpoint can be made available to funcx running outside of NeSI.\n",
    "\n",
    "For more details see: https://funcx.readthedocs.io/en/latest/endpoints.html.\n",
    "\n",
    "### Start the funcx endpoint on NeSI\n",
    "\n",
    "A default endpoint profile is created during the configure step above, which will suffice for us. We will be using funcx to submit jobs to Slurm or check the status of submitted jobs; no computationally expensive tasks should run directly on the endpoint itself.\n",
    "\n",
    "```sh\n",
    "# we are still on the Mahuika login node here...\n",
    "funcx-endpoint start\n",
    "```\n",
    "\n",
    "Now list your endpoints, confirm that the *default* endpoint is \"Active\" and make a note of your endpoint ID:\n",
    "\n",
    "```sh\n",
    "funcx-endpoint list\n",
    "+---------------+-------------+--------------------------------------+\n",
    "| Endpoint Name |   Status    |             Endpoint ID              |\n",
    "+===============+=============+======================================+\n",
    "| default       | Active      | 3abf6696-8ba4-4ac8-be69-c6c24031373d |\n",
    "+---------------+-------------+--------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store your funcx endpoint id here\n",
    "funcx_endpoint = \"3abf6696-8ba4-4ac8-be69-c6c24031373d\"  # my default endpoint on NeSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Globus guest collection on NeSI\n",
    "\n",
    "Create a guest collection on the NeSI endpoint, so that we don't need to do the NeSI two factor authentication repeatedly, we can just use Globus auth.\n",
    "\n",
    "Navigate to a directory under */nesi/nobackup/[project_code]/*, click sharing and add a shared collection. Make a note of the \"Endpoint UUID\". Also store the full path on NeSI to the shared collection you just created (`nesi_path`):\n",
    "\n",
    "https://transfer.nesi.org.nz/file-manager?origin_id=cc45cfe3-21ae-4e31-bad4-5b3e7d6a2ca1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store your NeSI endpoint and path here\n",
    "nesi_endpoint = \"f456a507-3c5b-41b9-9d7f-2315b9fed386\"  # my guest collection on NeSI\n",
    "nesi_path = \"/nesi/nobackup/nesi99999/csco212/funcx_demo\"  # the full path to where I created the guest collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Globus authentication on local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register an app with Globus, if you haven't done it already\n",
    "\n",
    "Note: I think this is a one off, you can reuse the same client id.\n",
    "\n",
    "> Navigate to the [Developer Site](https://developers.globus.org/) and select “Register your app with Globus.” You will be prompted to login – do so with the account you wish to use as your app’s administrator..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifier for the app we created on globus website above, can be reused\n",
    "CLIENT_ID = \"6ffc9c02-cf62-4268-a695-d9d100181962\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use fair-research-login to authenticate once with Globus for both FuncX and Globus transfer\n",
    "\n",
    "The first time you have to authenticate, then token is stored in mytokens.json and loaded from there on subsequent calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting login with Globus Auth, press ^C to cancel.\n",
      "Opening in existing browser session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1357587:1357587:0100/000000.911487:ERROR:sandbox_linux.cc(378)] InitializeSandbox() called with multiple threads in process gpu-process.\n"
     ]
    }
   ],
   "source": [
    "from fair_research_login import NativeClient, JSONTokenStorage\n",
    "\n",
    "cli = NativeClient(\n",
    "    client_id=CLIENT_ID,\n",
    "    token_storage=JSONTokenStorage('mytokens.json'),  # save/load tokens here\n",
    "    app_name=\"FuncX/Globus NeSI Demo\",\n",
    ")\n",
    "\n",
    "# get the requested scopes (load tokens from file if available, otherwise request new tokens)\n",
    "search_scope = \"urn:globus:auth:scope:search.api.globus.org:all\"  # for FuncX\n",
    "funcx_scope = \"https://auth.globus.org/scopes/facd7ccc-c5f4-42aa-916b-a0e270e2c2a9/all\"  # for FuncX\n",
    "openid_scope = \"openid\"  # for FuncX\n",
    "transfer_scope = \"urn:globus:auth:scope:transfer.api.globus.org:all\"  # for Globus transfer client\n",
    "https_scope = f\"https://auth.globus.org/scopes/{nesi_endpoint}/https\"  # for HTTPS upload/download to our guest collection on NeSI\n",
    "tokens = cli.login(\n",
    "    refresh_tokens=True,\n",
    "    requested_scopes=[openid_scope, search_scope, funcx_scope, transfer_scope, https_scope]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authorisers for requested scopes\n",
    "authorisers = cli.get_authorizers_by_scope(requested_scopes=[openid_scope, funcx_scope, search_scope, transfer_scope, https_scope])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start funcX client locally\n",
    "\n",
    "Start the funcX client locally so we can submit jobs to the NeSI funcX endpoint we just created. This will also require authentication with Globus Auth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcx.sdk.client import FuncXClient\n",
    "\n",
    "fxc = FuncXClient(\n",
    "    fx_authorizer=authorisers[funcx_scope],\n",
    "    search_authorizer=authorisers[search_scope],\n",
    "    openid_authorizer=authorisers[openid_scope],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcx.sdk.executor import FuncXExecutor\n",
    "\n",
    "# create a funcX executor\n",
    "funcx_executor = FuncXExecutor(fxc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connect to our Globus guest collection on NeSI\n",
    "\n",
    "Connect to the guest collection we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import globus_sdk\n",
    "\n",
    "tc = globus_sdk.TransferClient(authorizer=authorisers[transfer_scope])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Endpoint activated successfully using Globus Online credentials.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activate the NeSI endpoint\n",
    "res_nesi_ep = tc.endpoint_autoactivate(nesi_endpoint)\n",
    "assert res_nesi_ep['code'] != 'AutoActivationFailed'\n",
    "res_nesi_ep[\"message\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure HTTPS uploads/downloads for our NeSI guest collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint HTTPS base URL: https://g-dc68ab.c61f4.bd7c.data.globus.org\n"
     ]
    }
   ],
   "source": [
    "# get the base URL for uploads and downloads\n",
    "endpoint = tc.get_endpoint(nesi_endpoint)\n",
    "https_server = endpoint['https_server']\n",
    "print(f\"Endpoint HTTPS base URL: {https_server}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up authentication header\n",
    "# Globus SDK v2\n",
    "https_token_dict = cli.load_tokens_by_scope()[https_scope]\n",
    "https_auth_header = f\"{https_token_dict['token_type']} {https_token_dict['access_token']}\"\n",
    "# Globus SDK v3??\n",
    "#a = authorisers[https_scope]\n",
    "#https_auth_header = a.get_authorization_header()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download a file via https from the endpoint\n",
    "def download_file(remote_file, local_file):\n",
    "    import requests\n",
    "    import shutil\n",
    "    import time\n",
    "    import os\n",
    "\n",
    "    # file to download and URL\n",
    "    download_url = f\"{https_server}/{remote_file}\"\n",
    "    print(f\"Downloading: {download_url}\")\n",
    "\n",
    "    # authorisation\n",
    "    headers = {\n",
    "        \"Authorization\": https_auth_header,\n",
    "    }\n",
    "\n",
    "    # download\n",
    "    start_time = time.perf_counter()\n",
    "    with requests.get(download_url, headers=headers, stream=True) as r:\n",
    "        with open(local_file, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "    r.raise_for_status()\n",
    "    download_time = time.perf_counter() - start_time\n",
    "    file_size = os.path.getsize(local_file)\n",
    "    print(f\"Downloaded {local_file}: {file_size / 1024 / 1024:.3f} MB in {download_time:.3f} seconds ({file_size / 1024 / 1024 / download_time:.3f} MB/s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to upload a file via https to the endpoint\n",
    "def upload_file(local_file, remote_file):\n",
    "    import requests\n",
    "    import time\n",
    "    import os\n",
    "\n",
    "    # file to download and URL\n",
    "    upload_url = f\"{https_server}/{remote_file}\"\n",
    "    print(f\"Uploading: {upload_url}\")\n",
    "\n",
    "    # authorisation\n",
    "    headers = {\n",
    "        \"Authorization\": https_auth_header,\n",
    "    }\n",
    "\n",
    "    # upload\n",
    "    start_time = time.perf_counter()\n",
    "    with open(local_file, 'rb') as f:\n",
    "        r = requests.put(upload_url, data=f, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    upload_time = time.perf_counter() - start_time\n",
    "    file_size = os.path.getsize(local_file)\n",
    "    print(f\"Uploaded {local_file}: {file_size / 1024 / 1024:.3f} MB in {upload_time:.3f} seconds ({file_size / 1024 / 1024 / upload_time:.3f} MB/s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transfer input data to NeSI using Globus\n",
    "\n",
    "First we make a directory name that the simulation will be stored under, then copy the data under that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: 20220112T125541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransferResponse({'DATA_TYPE': 'mkdir_result', 'code': 'DirectoryCreated', 'message': 'The directory was created successfully', 'request_id': 'XFztddPaJ', 'resource': '/operation/endpoint/f456a507-3c5b-41b9-9d7f-2315b9fed386/mkdir'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a directory for running under\n",
    "from datetime import datetime\n",
    "\n",
    "# get a unique name for this run\n",
    "workdirbase = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "workdirname = workdirbase\n",
    "got_dirname = False\n",
    "existing_names = [item[\"name\"] for item in tc.operation_ls(nesi_endpoint, path=\"/\")]\n",
    "count = 0\n",
    "while not got_dirname:\n",
    "    # check the directory does not already exist\n",
    "    if workdirname in existing_names:\n",
    "        count += 1\n",
    "        workdirname = f\"{workdirbase}.{count:06d}\"\n",
    "    else:\n",
    "        got_dirname = True\n",
    "print(f\"Directory: {workdirname}\")\n",
    "tc.operation_mkdir(nesi_endpoint, workdirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/apoa1.pdb\n",
      "Uploaded input/apoa1.pdb: 6.773 MB in 7.535 seconds (0.899 MB/s)\n",
      "Uploading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/apoa1.namd\n",
      "Uploaded input/apoa1.namd: 0.001 MB in 5.767 seconds (0.000 MB/s)\n",
      "Uploading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/apoa1.psf\n",
      "Uploaded input/apoa1.psf: 12.855 MB in 4.300 seconds (2.990 MB/s)\n",
      "Uploading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/par_all22_popc.xplor\n",
      "Uploaded input/par_all22_popc.xplor: 0.000 MB in 3.541 seconds (0.000 MB/s)\n",
      "Uploading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/par_all22_prot_lipid.xplor\n",
      "Uploaded input/par_all22_prot_lipid.xplor: 0.149 MB in 5.846 seconds (0.025 MB/s)\n",
      "Uploading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/run.sl\n",
      "Uploaded input/run.sl: 0.000 MB in 2.560 seconds (0.000 MB/s)\n",
      "transferring source files to NeSI is complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# can only upload files using HTTPS?\n",
    "# get the list of source files\n",
    "src_input_path = \"input\"\n",
    "source_files = [f for f in os.listdir(src_input_path) if os.path.isfile(os.path.join(src_input_path, f))]\n",
    "\n",
    "# transfer the source files\n",
    "for source_file in source_files:\n",
    "    upload_file(os.path.join(src_input_path, source_file), f\"{workdirname}/{source_file}\")\n",
    "\n",
    "print(\"transferring source files to NeSI is complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run the processing using funcX\n",
    "\n",
    "Two functions are called using FuncX:\n",
    "\n",
    "1. Submit job to Slurm\n",
    "2. Check Slurm job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FuncX endpoint id: 3abf6696-8ba4-4ac8-be69-c6c24031373d\n"
     ]
    }
   ],
   "source": [
    "print(f\"FuncX endpoint id: {funcx_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple test function that returns the hostname where the endpoint is running, just as a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing done? False\n",
      "processing done? True\n",
      "FuncX endpoint is running on: mahuika01\n"
     ]
    }
   ],
   "source": [
    "# test function to see if things are working\n",
    "def test_function():\n",
    "    import socket\n",
    "    return socket.gethostname()\n",
    "\n",
    "# With the executor, functions are auto-registered\n",
    "future = funcx_executor.submit(test_function, endpoint_id=funcx_endpoint)\n",
    "\n",
    "# You can check status of your task without blocking\n",
    "print(\"processing done?\", future.done())\n",
    "\n",
    "# Block and wait for the result:\n",
    "result = future.result()\n",
    "\n",
    "print(\"processing done?\", future.done())\n",
    "\n",
    "print(f\"FuncX endpoint is running on: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the 2 Slurm functions for interacting with Slurm (if the Slurm API was available we could use that instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that submits a job to Slurm (assumes submit script and other required inputs were uploaded via Globus)\n",
    "def submit_slurm_job(submit_script, work_dir=None):\n",
    "    \"\"\"Runs the given command in a Slurm job.\"\"\"\n",
    "    # have to load modules within the function\n",
    "    import os\n",
    "    import subprocess\n",
    "    \n",
    "    # change to working directory\n",
    "    if work_dir is not None and os.path.isdir(work_dir):\n",
    "        os.chdir(work_dir)\n",
    "        \n",
    "    print(os.listdir())\n",
    "    \n",
    "    # submit the Slurm job and return the job id\n",
    "    submit_cmd = f'sbatch --priority=9999 {submit_script}'\n",
    "    with open(\"submit_cmd.txt\", \"w\") as fh:\n",
    "        fh.write(submit_cmd + \"\\n\")\n",
    "    output = subprocess.check_output(submit_cmd, shell=True, universal_newlines=True)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that checks Slurm job status\n",
    "def check_slurm_job_status(jobid):\n",
    "    \"\"\"Check Slurm job status.\"\"\"\n",
    "    # have to load modules within the function\n",
    "    import subprocess\n",
    "    \n",
    "    # query the status of the job using sacct\n",
    "    cmd = f'sacct -j {jobid} -X -o State -n'\n",
    "    output = subprocess.check_output(cmd, shell=True, universal_newlines=True)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job to Slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job submitted: 23826177\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# full path on NeSI to where the files were uploaded\n",
    "nesi_full_path = os.path.join(nesi_path, workdirname)\n",
    "\n",
    "# With the executor, functions are auto-registered\n",
    "future = funcx_executor.submit(submit_slurm_job, \"run.sl\", endpoint_id=funcx_endpoint, work_dir=nesi_full_path)\n",
    "\n",
    "# Block and wait for the result:\n",
    "try:\n",
    "    result = future.result()\n",
    "except subprocess.CalledProcessError as exc:\n",
    "    print(\"submitting job failed:\")\n",
    "    print(f\"    return code: {exc.returncode}\")\n",
    "    print(f\"    cmd: {exc.cmd}\")\n",
    "    print(f\"    output: {exc.output}\")\n",
    "\n",
    "# get the Slurm Job ID\n",
    "jobid = result.split()[-1]\n",
    "print(f\"Job submitted: {jobid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking Slurm job status via funcX: RUNNING\n",
      "checking Slurm job status via funcX: RUNNING\n",
      "checking Slurm job status via funcX: COMPLETED\n",
      "Job finished\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "job_finished = False\n",
    "while not job_finished:\n",
    "    future = funcx_executor.submit(check_slurm_job_status, jobid, endpoint_id=funcx_endpoint)\n",
    "    print(\"checking Slurm job status via funcX: \", end=\"\")\n",
    "    result = future.result()\n",
    "    job_status = result.strip()\n",
    "    print(job_status)\n",
    "    if job_status not in (\"RUNNING\", \"PENDING\"):  # TODO: check possible statuses\n",
    "        job_finished = True\n",
    "    time.sleep(5)\n",
    "print(\"Job finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Copy results back using Globus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/apoa1.namd\n",
      "Downloaded output/20220112T125541/apoa1.namd: 0.001 MB in 4.362 seconds (0.000 MB/s)\n",
      "Downloading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/apoa1.pdb\n",
      "Downloaded output/20220112T125541/apoa1.pdb: 6.773 MB in 5.003 seconds (1.354 MB/s)\n",
      "Downloading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/apoa1.psf\n",
      "Downloaded output/20220112T125541/apoa1.psf: 12.855 MB in 4.008 seconds (3.207 MB/s)\n",
      "Downloading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/par_all22_popc.xplor\n",
      "Downloaded output/20220112T125541/par_all22_popc.xplor: 0.000 MB in 2.460 seconds (0.000 MB/s)\n",
      "Downloading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/par_all22_prot_lipid.xplor\n",
      "Downloaded output/20220112T125541/par_all22_prot_lipid.xplor: 0.149 MB in 6.085 seconds (0.024 MB/s)\n",
      "Downloading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/run.sl\n",
      "Downloaded output/20220112T125541/run.sl: 0.000 MB in 2.750 seconds (0.000 MB/s)\n",
      "Downloading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/slurm-23826177.out\n",
      "Downloaded output/20220112T125541/slurm-23826177.out: 0.183 MB in 6.213 seconds (0.029 MB/s)\n",
      "Downloading: https://g-dc68ab.c61f4.bd7c.data.globus.org/20220112T125541/submit_cmd.txt\n",
      "Downloaded output/20220112T125541/submit_cmd.txt: 0.000 MB in 2.935 seconds (0.000 MB/s)\n",
      "transferring results from NeSI is complete: output/20220112T125541\n"
     ]
    }
   ],
   "source": [
    "# create directory for storing result\n",
    "store_path = os.path.join(\"output\", workdirname)\n",
    "os.mkdir(store_path)\n",
    "\n",
    "# list and download files\n",
    "ls = tc.operation_ls(nesi_endpoint, path=workdirname)\n",
    "for item in ls:\n",
    "    if item[\"type\"] == \"file\":\n",
    "        fn = item[\"name\"]\n",
    "        download_file(f\"{workdirname}/{fn}\", os.path.join(store_path, fn))\n",
    "    else:\n",
    "        print(f\"Skipping: {item['name']} (can only download files over HTTPS)\")\n",
    "\n",
    "print(f\"transferring results from NeSI is complete: {store_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes about FuncX so far\n",
    "\n",
    "* above uses FuncX just to submit a Slurm job and then poll for completion (could also be done with Slurm API if that was made available)\n",
    "* researcher needs to manually run a funcx endpoint on NeSI (and keep it running there)\n",
    "  - eventually should be integrated with Globus federated endpoint?\n",
    "  - this runs an endpoint on the login node\n",
    "  - could be a pain if the endpoint is killed for some reason and the user needs to reconnect and start it again\n",
    "* FuncX does know about Slurm too, so you could set FuncX up to directly run your function in a Slurm job without having to submit anything separately, see snippet from an endpoint config.py:\n",
    "  ```sh\n",
    "    from funcx_endpoint.endpoint.utils.config import Config\n",
    "    from parsl.providers import LocalProvider, SlurmProvider\n",
    "\n",
    "    config = Config(\n",
    "        scaling_enabled=True,\n",
    "        provider=SlurmProvider(\n",
    "            \"large\",\n",
    "            min_blocks=1,\n",
    "            max_blocks=1,\n",
    "            nodes_per_block=1,\n",
    "            cores_per_node=2,\n",
    "            mem_per_node=16,\n",
    "            exclusive=False,\n",
    "            cmd_timeout=120,\n",
    "            walltime='2:00:00',\n",
    "        ),\n",
    "        #max_workers_per_node=2,\n",
    "        funcx_service_address='https://api.funcx.org/v1'\n",
    "    )\n",
    "  ```\n",
    "* reasons for not using FuncX SlurmProvider directly currently\n",
    "  - funcx currently has no way to know how much work a function may involve\n",
    "    - could lead to failures due to wall time exceeded, etc.\n",
    "  - not \"elastic\"\n",
    "    - have to start a new endpoint if need more resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
